# Optimization Algorithms Project üöÄ

Welcome to the **Optimization Algorithms Project**! This repository showcases a comprehensive implementation and visualization of various first-order and second-order optimization algorithms, benchmarked using the **Rosenbrock Function**. The project demonstrates the convergence behavior of these algorithms through detailed 2D and 3D visualizations.

## About the Project üìò

This project was developed by **Shubhendu Shukla**, leveraging knowledge gained through coursework and independent research. The goal was to implement optimization algorithms from scratch, compare their performance, and gain insights into their behavior under different conditions.

The project features:

- **11 First-Order Optimization Algorithms**
- **3 Second-Order Optimization Algorithms**
- Comprehensive visualizations for each algorithm's convergence path
- Benchmarking using the Rosenbrock Function, a standard in optimization studies

---

## Repository Structure üìÇ

The repository is organized into the following main folders:

### `First_Order_Optimization/`
Contains implementations of first-order optimization algorithms:
- **Adam Algorithm**
- **Adagrad Algorithm**
- **Basic Gradient Descent**
- **EDMA (Exponential Decay Method for Adaptation)**
- **Gradient Descent with Armijo Line Search**
- **Linear Regression**
- **Logistic Regression**
- **Momentum**
- **Nesterov Accelerated Gradient**
- **RMSProp**
- **Subgradient Method**

### `Second_Order_Optimization/`
Contains implementations of second-order optimization algorithms:
- **BFGS (Broyden-Fletcher-Goldfarb-Shanno)**
- **Conjugate Gradient Method**
- **Newton‚Äôs Method**

### `Visualizations/`
This folder contains scripts and output files for 2D and 3D visualizations of the optimization paths, created using libraries like Matplotlib and Plotly.

---

## Benchmark Function üß™

The **Rosenbrock Function** serves as the benchmark:

\[
f(x, y) = (1 - x)^2 + 100(y - x^2)^2
\]

### Why the Rosenbrock Function?
The function features:
- A narrow, curved valley leading to a global minimum
- Challenges for optimization algorithms in terms of convergence and robustness

---

## Key Features ‚ú®

- **Implementation From Scratch**: All algorithms were coded manually to ensure a deep understanding of their inner workings.
- **Comprehensive Visualizations**: The convergence paths of algorithms are displayed with stunning 2D and 3D plots.
- **Educational Focus**: Each algorithm is documented with detailed comments and explanations.
- **Algorithm Comparison**: Insights into the strengths and weaknesses of each algorithm.

---

## Setup & Usage üõ†Ô∏è

### Prerequisites
- Python 3.x
- Libraries: `numpy`, `matplotlib`, `plotly`, `scipy`

### Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/shuklaShubhendu/Optimization-Algorithms.git
   cd Optimization-Algorithms

## Contribution üåü

This project is the culmination of my individual efforts, reflecting my growth as a software developer and my passion for optimization and algorithm design. While this version is complete, I'm open to feedback and suggestions for further improvement.

## Future Work üõ§Ô∏è

Adding additional benchmark functions for broader comparisons.
Implementing other advanced optimization algorithms like L-BFGS and Simulated Annealing.
Extending visualizations to interactive web-based platforms using Plotly Dash or Streamlit.
Why they dont see any changes appmod

## License üìú
This project is licensed under the MIT License. Feel free to use, modify, and distribute the code with proper attribution.

## Contact üì¨
Have questions or feedback? Feel free to reach out:


Name: Shubhendu Shukla
Email: shuklashubhendu6@gmail.com
GitHub: @shuklaShubhendu
Thank you for exploring my project! üòä
